"""
Extracts token representations, using examples from pickled_subsets_dir
(generated by subset_examples.py, see readme). Outputs an npy tensor
(shape: num_tokens x hidden_size) for each language.

python3 extract_representations.py --model_name_or_path="xlm-roberta-base" \
--per_device_eval_batch_size=8 --max_seq_length 512 --cache_dir="../hf_cache" \
--pickled_subsets_dir="../oscar_xlmr_tokenized_subsets/for_visualizations" \
--output_dir="../oscar_xlmr_reps" \
--layer=8 --langs ar en es zh ru

"""

import os
import argparse
import numpy as np
import torch
import codecs
import pickle
from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelForMaskedLM,
)

from src.utils import get_hidden_states


def create_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_name_or_path', type=str)
    parser.add_argument('--cache_dir', type=str) # Where to cache the downloaded model.
    parser.add_argument('--pickled_subsets_dir', type=str)
    parser.add_argument('--output_dir', type=str)
    parser.add_argument('--per_device_eval_batch_size', type=int, default=8)
    parser.add_argument('--max_seq_length', type=int, default=512)
    parser.add_argument('--do_lower_case', type=bool, default=False)
    parser.add_argument('--layer', type=int)
    parser.add_argument('--langs', nargs='+', type=str)
    return parser


def main(args):
    # Load model.
    print("Loading model...")
    config = AutoConfig.from_pretrained(
        args.model_name_or_path,
        cache_dir=args.cache_dir,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        do_lower_case=args.do_lower_case,
        cache_dir=args.cache_dir,
    )
    model_class = AutoModelForMaskedLM
    model = model_class.from_pretrained(
        args.model_name_or_path,
        config=config,
        cache_dir=args.cache_dir,
    )

    # Extract representations.
    for lang in args.langs:
        print("Running language {}.".format(lang))
        # Load examples.
        pickled_subsets_file = os.path.join(args.pickled_subsets_dir, "{}.pickle".format(lang))
        if os.path.isfile(pickled_subsets_file):
            print("Loading example subsets from file: {}".format(pickled_subsets_file))
            with open(pickled_subsets_file, 'rb') as handle:
                examples_subsets = pickle.load(handle)
            examples = examples_subsets[0] # By default, use the first subset.
            for example_i in range(len(examples)): # Truncate examples.
                if len(examples[example_i]) > args.max_seq_length:
                    examples[example_i] = examples[example_i][:args.max_seq_length]
        else:
            print("Missing pickled examples file ({}). Skipping.".format(lang))
            continue
        # Run the model.
        total_batch_size = args.per_device_eval_batch_size
        if torch.cuda.is_available():
            total_batch_size = args.per_device_eval_batch_size * torch.cuda.device_count()
            model = model.cuda()
        if torch.cuda.device_count() > 1 and not isinstance(model, torch.nn.DataParallel):
            model = torch.nn.DataParallel(model)
        # Get hidden states with shape: num_tokens x hidden_size.
        print("Running model...")
        hidden_states = get_hidden_states(model, examples, total_batch_size, tokenizer, args.layer)
        if isinstance(model, torch.nn.DataParallel):
            model = model.module
        # Save representations.
        rep_outpath = os.path.join(args.output_dir, "{0}_layer{1}_reps.npy".format(lang, args.layer))
        np.save(rep_outpath, hidden_states, allow_pickle=False)
        print("Saved representations.")
    print("Done.")


if __name__ == "__main__":
    parser = create_parser()
    args = parser.parse_args()
    main(args)
